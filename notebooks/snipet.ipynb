{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"./.env.local\")\n",
    "\n",
    "print(os.getenv('AWS_ACCESS_KEY_ID'))\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean chromadb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db\")\n",
    "vectorstore.delete_collection()\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'documents' supprim√©e.\n"
     ]
    }
   ],
   "source": [
    "from chromadb import PersistentClient\n",
    "\n",
    "persist_directory = \"./chroma_db\"\n",
    "client = PersistentClient(path=persist_directory)\n",
    "collection = client.get_or_create_collection(name=\"documents\")\n",
    "client.delete_collection(name=\"documents\")\n",
    "print(f\"Collection 'documents' supprim√©e.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def pull_model():\n",
    "    print(f\"--- Pulling {model} model ---\")\n",
    "    subprocess.run([\"ollama\", \"pull\", model], check=True)\n",
    "    print(f\"Model {model} pulled successfully.\")\n",
    "\n",
    "def run_model():\n",
    "    print(f\"--- Running {model} model ---\")\n",
    "    subprocess.run([\"ollama\", \"run\", model], check=True)\n",
    "    print(f\"Model {model} is now running.\")\n",
    "\n",
    "pull_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ollama\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Load Documents\n",
    "print(\"--- Step 1 : Upload document ---\")\n",
    "with open('./assets/ressources/base.txt', 'r', encoding='utf-8') as fichier:\n",
    "    doc_content = fichier.read()\n",
    "\n",
    "# Split - Chunking\n",
    "print(\"--- Step 2 : Chunking document ---\")\n",
    "def chunk_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - chunk_overlap \n",
    "    return chunks\n",
    "\n",
    "splits = chunk_text(doc_content)\n",
    "\n",
    "print(f\"‚úÖ Document d√©coup√© en {len(splits)} chunks.\")\n",
    "\n",
    "# Embeddings Model\n",
    "print(\"--- Step 3 : Upload embeddings model and Creating embeddings ---\")\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.encode(splits).tolist()\n",
    "\n",
    "print(f\"‚úÖ {len(embeddings)} embeddings g√©n√©r√©s.\")\n",
    "\n",
    "\n",
    "# Create ChromaDB database\n",
    "print(\"--- Step 4 : Create ChromaDB database ---\")\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = client.get_or_create_collection(name=\"documents\")\n",
    "\n",
    "# üìå Ajouter les embeddings √† la base de donn√©es\n",
    "print(\"--- Step 4 : Storing embeddings in ChromaDB ---\")\n",
    "collection.add(\n",
    "    ids=[str(i) for i in range(len(splits))],\n",
    "    documents=splits,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "print(\"‚úÖ Documents enregistr√©s dans ChromaDB.\")\n",
    "\n",
    "\n",
    "def retrieve_documents(query, top_k=3):\n",
    "    query_embedding = embedding_model.encode([query]).tolist()[0]\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "\n",
    "    retrieved_docs = results[\"documents\"][0]\n",
    "    return retrieved_docs\n",
    "\n",
    "def generate_response(context, question):\n",
    "    full_prompt = f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=\"mistral:7b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def rag_pipeline(question):\n",
    "    print(\"\\n--- R√©cup√©ration des documents ---\")\n",
    "    retrieved_docs = retrieve_documents(question)\n",
    "    context = \"\\n\\n\".join(retrieved_docs)\n",
    "\n",
    "    print(f\"Documents r√©cup√©r√©s : {retrieved_docs}\\n\")\n",
    "\n",
    "    print(\"\\n--- G√©n√©ration de r√©ponse ---\")\n",
    "    response = generate_response(context, question)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "question = \"Can you list the team members of M-Motors?\"\n",
    "response = rag_pipeline(question)\n",
    "\n",
    "print(\"\\n--- R√©ponse finale ---\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Load Documents\n",
    "print(\"--- Step 1 : Upload document ---\")\n",
    "with open('./assets/ressources/base.txt', 'r', encoding='utf-8') as fichier:\n",
    "    doc_content = fichier.read()\n",
    "\n",
    "# Convert doc in langchaindoc format\n",
    "print(\"--- Step 2 : Convert document in Langchain document format ---\")\n",
    "docs = [Document(page_content=doc_content)]\n",
    "\n",
    "# Split - Chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embeddings Model\n",
    "print(\"--- Step 3 : Upload embeddings model ---\")\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\" \n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "print(f\"Upload finish : {model_name}\")\n",
    "\n",
    "# Create ChromaDB database\n",
    "print(\"--- Step 4 : Create ChromaDB database ---\")\n",
    "vectorstore = Chroma.from_documents(splits, embedding=embeddings, persist_directory=\"./chroma_db\")\n",
    "\n",
    "# Retriever\n",
    "print(\"--- Step 5 : Create retriever ---\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Persiste data in ChromaDB\n",
    "print(\"--- Step 6 : Persiste data in ChromaDB ---\")\n",
    "vectorstore.persist()\n",
    "\n",
    "print(\"\\n--- Verifications ---\")\n",
    "print(f\"Document content :\\n{doc_content}\\n\")\n",
    "print(f\"Object document create :\\n{docs}\\n\")\n",
    "print(f\"Number of chunks create : {len(splits)}\")\n",
    "for i, chunk in enumerate(splits):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(chunk)\n",
    "print(f\"\\n\")\n",
    "print(f\"Object Embeddings : \\n{embeddings}\\n\")\n",
    "print(f\"Object VectorStore : \\n{vectorstore}\\n\")\n",
    "print(f\"Object Retriever : \\n{retriever}\\n\")\n",
    "\n",
    "\n",
    "# Define Prompt Template\n",
    "print(\"--- Step 1: Defining prompt model ---\")\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "print(f\"Prompt model : \\n{prompt_template}\\n\")\n",
    "\n",
    "# Load LLM Model\n",
    "print(\"\\n--- Step 2: Loading LLM model ---\")\n",
    "llm = Ollama(model=model)\n",
    "print(f\"{model} model successfully loaded.\")\n",
    "\n",
    "# Format docs\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# RAG chaine : Retrieval + Generation\n",
    "def rag_chain(question, retriever):\n",
    "\n",
    "    # Retrieved \n",
    "    print(\"\\n--- Executing RAG Pipeline ---\")\n",
    "\n",
    "    docs = retriever.invoke(question)\n",
    "    context = format_docs(docs)\n",
    "\n",
    "    print(f\"Retrieved {len(docs)} documents.\")\n",
    "    print(f\"Retrieved documents :\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"\\Doc {i+1}:\")\n",
    "        print(doc)\n",
    "    print(f\"\\n\")\n",
    "\n",
    "    print(f\"Context : \\n{context}\\n\")\n",
    "    \n",
    "    # Build prompt\n",
    "    full_prompt = prompt_template.format(question=question, context=context)\n",
    "    print(f\"Prompt : \\n{full_prompt}\\n\")\n",
    "\n",
    "    # Execution\n",
    "    response = chat(\n",
    "        model=\"mistral:7b\",\n",
    "        messages=[{'role': 'user', 'content': full_prompt}],\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    return StrOutputParser().parse(response[\"message\"][\"content\"])\n",
    "\n",
    "question = \"Can you list the team members of M-Motors?\"\n",
    "print(\"\\n--- Testing RAG Query ---\")\n",
    "response = rag_chain(question, retriever)\n",
    "\n",
    "print(\"\\n--- Final Response ---\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
