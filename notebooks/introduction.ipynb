{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question ? \n",
    "\n",
    "Architecture du modèle\n",
    "\n",
    "Méthode d’entraînement\n",
    "\n",
    "GPT BERT\n",
    "\n",
    "\n",
    "Qualité et taille des données d’entraînement\n",
    "Nombre de paramètres\n",
    "\n",
    "quel sont les métrique de performance comment les mettre en place et les analyser ? \n",
    "Comment fine-tuning un model ? \n",
    "Banchmark MMLU, GLUE ou SuperGLUE ? \n",
    "\n",
    "\n",
    "AI agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "RAG stands for Retrieval Augmented Generation.\n",
    "\n",
    "![RAG Overview](./assets/images/rag_concepts.png)\n",
    "\n",
    "It was introduced in the paper [*Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*](https://arxiv.org/abs/2005.11401).\n",
    "\n",
    "Each step can be roughly broken down to:\n",
    "\n",
    "* **Retrieval** - Seeking relevant information from a source given a query. For example, getting relevant passages of Wikipedia text from a database given a question.\n",
    "* **Augmented** - Using the relevant retrieved information to modify an input to a generative model (e.g. an LLM).\n",
    "* **Generation** - Generating an output given an input. For example, in the case of an LLM, generating a passage of text given an input prompt.\n",
    "\n",
    "\n",
    "![RAG Pipeline](./assets/images/rag_pipeline.png)\n",
    "\n",
    "\n",
    "## Why RAG?\n",
    "\n",
    "The main goal of RAG is to improve the generation outptus of LLMs.\n",
    "\n",
    "Two primary improvements can be seen as:\n",
    "1. **Preventing hallucinations** \n",
    "2. **Work with custom data**\n",
    "\n",
    "## Why RAG use for?\n",
    "\n",
    "For example you could use RAG for:\n",
    "* **Customer support Q&A chat** \n",
    "* **Email chain analysis**\n",
    "* **Company internal documentation chat**\n",
    "* **Textbook Q&A** \n",
    "\n",
    "## Terms\n",
    "\n",
    "**Token** A piece of text\n",
    "\n",
    "**Embedding** A learned numerical representation (vector) of a piece of data. \n",
    "\n",
    "**Embedding model** A model designed to accept input data and output a numerical representation.\n",
    "\n",
    "**Similarity search/vector search** Similarity search/vector search aims to find two vectors which are close together in high-demensional space.\n",
    "\n",
    "**Large Language Model (LLM)** A model which has been trained to numerically represent the patterns in text.\n",
    "\n",
    "**LLM context window** The number of tokens a LLM can accept as input. For example, as of March 2024, GPT-4 has a default \n",
    "context window of 32k tokens. In a RAG pipeline, if a model has a larger context window, it can accept more reference items.\n",
    "\n",
    "**Prompt** A common term for describing the input to a generative LLM.\n",
    "\n",
    "Basic RAG steps : \n",
    "- Setup \n",
    "  - Set environment variables\n",
    "  - Setup embedding model\n",
    "  - Create vector database\n",
    "  - Setup llm model\n",
    "  - Define Prompt Template\n",
    "- Pre Indexing\n",
    "  - Load Documents\n",
    "  - Cleaning & formatting\n",
    "  - Metadata extraction\n",
    "- Indexing\n",
    "  - Chunking\n",
    "  - Embedding\n",
    "  - Persist data\n",
    "  - Create retriever\n",
    "- Retrieval\n",
    "  - Retrieve documents \n",
    "  - Format documents\n",
    "- Generation\n",
    "  - Build prompt\n",
    "  - Generate Response\n",
    "\n",
    "\n",
    "## Lanscape \n",
    "\n",
    "![RAG advanced Overview](./assets/images/advanced_overview.png)\n",
    "\n",
    "Architecture détaillée d'un système de *Retrieval-Augmented Generation* (RAG)  \n",
    "\n",
    "### 1. **Query Translation (Traduction de la requête)**\n",
    "Cette section vise à transformer la question initiale en une forme plus adaptée à la récupération d’informations.  \n",
    "- **Techniques utilisées** :  \n",
    "  - *Multi-query* : Décompose une question en plusieurs sous-questions.  \n",
    "  - *RAG-Fusion* : Fusionne les résultats de différentes requêtes pour améliorer la qualité des réponses.  \n",
    "  - *Décomposition* : Scinde une requête complexe en plusieurs étapes plus simples.  \n",
    "  - *Step-back* : Reformule la question pour obtenir des résultats plus pertinents.  \n",
    "  - *HyDE* (*Hypothetical Document Embeddings*) : Génère des documents hypothétiques basés sur la question pour améliorer la recherche.  \n",
    "\n",
    "### 2. **Routing (Routage de la requête)**\n",
    "Cette section détermine le meilleur moyen d’accéder aux informations demandées.  \n",
    "- **Routage logique** :  \n",
    "  - Un LLM choisit la base de données (*Relational DB*, *Vector DB*, *Graph DB*) en fonction de la requête.  \n",
    "- **Routage sémantique** :  \n",
    "  - La question est encodée (*embedding*) et comparée à différents modèles de prompts pour sélectionner le plus adapté.  \n",
    "\n",
    "### 3. **Query Construction (Construction de la requête)**\n",
    "Cette section traduit la question en une requête adaptée à la base de données utilisée.  \n",
    "- **Relational DBs** : Convertit la requête en SQL via *Text-to-SQL*, notamment avec *PGVector*.  \n",
    "- **GraphDBs** : Utilise *Text-to-Cypher* pour interagir avec des bases orientées graphe.  \n",
    "- **VectorDBs** : Un *Self-query retriever* génère automatiquement des filtres de métadonnées pour optimiser la recherche.  \n",
    "\n",
    "### 4. **Indexing (Indexation des documents)**\n",
    "L’indexation permet d’organiser et de structurer les données pour améliorer la récupération d’informations.  \n",
    "- **Optimisation des segments (Chunk Optimization)** : Divise les documents en segments optimaux en fonction des caractères, sections, et délimiteurs sémantiques (*Semantic Splitter*).  \n",
    "- **Indexation multi-représentation** :  \n",
    "  - Convertit les documents en unités plus compactes, comme des résumés (*Parent Document, Dense X*).  \n",
    "- **Embeddings spécialisés** :  \n",
    "  - Utilise des modèles avancés comme *ColBERT* pour un encodage sémantique amélioré.  \n",
    "- **Indexation hiérarchique** :  \n",
    "  - Implémente une organisation en arbre pour résumer les documents à différents niveaux d’abstraction (*RAPTOR*).  \n",
    "\n",
    "\n",
    "### 5. **Retrieval (Récupération de l'information)**\n",
    "Cette phase consiste à rechercher les documents les plus pertinents et à les affiner.  \n",
    "- **Classement des documents** (*Ranking*) :  \n",
    "  - Utilisation de *Re-Rank*, *RankGPT*, et *RAG-Fusion* pour trier et filtrer les documents selon leur pertinence.  \n",
    "- **Affinement** (*Refinement*) :  \n",
    "  - Compression des documents pour en extraire l’essentiel (via *CRAG*).  \n",
    "- **Récupération active** (*Active Retrieval*) :  \n",
    "  - Recherche supplémentaire dans de nouvelles sources si les résultats initiaux sont jugés insuffisants.  \n",
    "\n",
    "### 6. **Generation (Génération de réponse)**\n",
    "Après la récupération des documents pertinents, un modèle génératif produit la réponse finale.  \n",
    "- **Self-RAG, RRR (Response Refinement & Retrieval)** :  \n",
    "  - Ajuste la qualité des réponses en réécrivant la question ou en récupérant d’autres documents.  \n",
    "\n",
    "Cette architecture optimise la pertinence et la qualité des réponses en combinant plusieurs stratégies de recherche et de traitement des données.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM MODEL\n",
    "\n",
    "| Entreprise | Modèle         | Nombre de paramètres | Fenêtre de contexte | Licence      | Remarques                                                                      |\n",
    "|------------|----------------|----------------------|---------------------|--------------|--------------------------------------------------------------------------------|\n",
    "| **Mistral**| Mistral 7B     | 7 milliards          | Jusqu’à 32 k tokens | Open source  | Conçu pour offrir de bonnes performances tout en restant accessible en ressources.|\n",
    "\n",
    "### Ollama commands\n",
    "\n",
    "```bash\n",
    "ollama pull mistral:7b \n",
    "```\n",
    "\n",
    "```bash\n",
    "ollama run mistral:7b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def pull_model(model):\n",
    "    print(f\"--- Pulling {model} model ---\")\n",
    "    subprocess.run([\"ollama\", \"pull\", model], check=True)\n",
    "    print(f\"Model {model} pulled successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pulling mistral:7b model ---\n",
      "Model mistral:7b pulled successfully.\n"
     ]
    }
   ],
   "source": [
    "model=\"mistral:7b\"\n",
    "pull_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ollama\n",
    "%pip install chromadb\n",
    "%pip install langchain\n",
    "%pip install langchain-core\n",
    "%pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### STEP 1: Setup ####\n",
      "[INFO] Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "[SUCCESS] Embedding model loaded in 2.74s\n",
      "[SUCCESS] 2.74s\n",
      "\n",
      "#### STEP 2: Pre-Indexing ####\n",
      "[INFO] Loading document from ./assets/ressources/base.txt\n",
      "[SUCCESS] Document loaded. Length: 4388 characters. 0.00s\n",
      "[INFO] Chunking document...\n",
      "[SUCCESS] Document split into 6 chunks. 0.01s\n",
      "[SUCCESS] 0.01s\n",
      "\n",
      "#### STEP 3: Indexing ####\n",
      "[INFO] Storing embeddings in ChromaDB\n",
      "[SUCCESS] Documents stored in ChromaDB. 8.29s\n",
      "[SUCCESS] 8.29s\n",
      "\n",
      "#### STEP 4: Retrieval & Generation ####\n",
      "[INFO] Creating retriever\n",
      "[INFO] Defining prompt template...\n",
      "[SUCCESS] Prompt template generated. 0.02s\n",
      "[INFO] Generating response...\n",
      "[SUCCESS] Response generated. 553.79s\n",
      "[SUCCESS] 553.81s\n",
      "\n",
      "#### FINAL RESPONSE ####\n",
      " M-Motors is a successful and highly reputable company specializing in the selling of used vehicles, having become one of the top 10 companies nationwide after 30 years. The company offers a diverse range of brands, models, engine types, mileage options, and price points to cater to all customer needs and budgets. M-Motors places great emphasis on providing high-quality customer service by offering thorough technical inspections, repairs, refurbishments, warranty options, personalized advice, test drives, financing services, trade-in services, and a new long-term leasing option with a purchase option (LLD/LOA) which includes comprehensive insurance, breakdown assistance, maintenance, after-sales service, technical inspections, and more. The company employs around 800 staff members and serves approximately one million customers nationwide. The IT department is in the process of revamping the current web application to facilitate the purchase of used cars with new features such as vehicle search (for both purchase and rental), registration and submission of applications, document upload via an interface for a paperless process, and more.\n",
      "[SUCCESS] 0.00s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from ollama import chat\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "#### SETUP ####\n",
    "\n",
    "def load_embedding_model(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Load the embedding model for text vectorization.\n",
    "    :param model_name: Name of the SentenceTransformer model to load.\n",
    "    :return: Loaded embedding model instance.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Loading embedding model: {model_name}\")\n",
    "    start_time = time.time()\n",
    "    model_kwargs = {\"device\": \"cpu\"}\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[SUCCESS] Embedding model loaded in {elapsed_time:.2f}s\")\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "#### PRE-INDEXING ####\n",
    "\n",
    "def load_document(file_path):\n",
    "    \"\"\"\n",
    "    Load document content from a file.\n",
    "    :param file_path: Path to the document.\n",
    "    :return: String content of the file.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Loading document from {file_path}\")\n",
    "    start_time = time.time()\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[SUCCESS] Document loaded. Length: {len(content)} characters. {elapsed_time:.2f}s\")\n",
    "    return content\n",
    "\n",
    "\n",
    "def chunk_text(doc_content, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    :param doc_content: Input text.\n",
    "    :param chunk_size: Size of each chunk.\n",
    "    :param chunk_overlap: Overlap between consecutive chunks.\n",
    "    :return: List of text chunks.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Chunking document...\")\n",
    "    start_time = time.time()\n",
    "    docs = [Document(page_content=doc_content)]\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[SUCCESS] Document split into {len(chunks)} chunks. {elapsed_time:.2f}s\")\n",
    "    return chunks\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "def create_vectorstore_indexing_chunks(chunks, embeddings, persist_directory=\"./chroma_db\"):\n",
    "    \"\"\"\n",
    "    Create chromaDB database, embeded chunks and Store embeddings.\n",
    "    :param chunks: List of text chunks.\n",
    "    :param embeddings: List of embeddings.\n",
    "    :param persist_directory: Path where the database is stored.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Storing embeddings in ChromaDB\")\n",
    "    start_time = time.time()\n",
    "    vectorstore = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=persist_directory)\n",
    "    vectorstore.persist()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[SUCCESS] Documents stored in ChromaDB. {elapsed_time:.2f}s\")\n",
    "    return vectorstore\n",
    "\n",
    "#### RETRIEVAL ####\n",
    "\n",
    "def create_retriever(vectorstore):\n",
    "    \"\"\"Create a retriever from the ChromaDB vector store.\"\"\"\n",
    "    print(\"[INFO] Creating retriever\")\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "#### GENERATION ####\n",
    "\n",
    "def define_prompt_template():\n",
    "    \"\"\"Define the prompt template for the LLM.\"\"\"\n",
    "    print(\"[INFO] Defining prompt template...\")\n",
    "    start_time = time.time()\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[SUCCESS] Prompt template generated. {elapsed_time:.2f}s\")\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def generate_response(question, retriever, prompt_template):\n",
    "    \"\"\"\n",
    "    Generate a response based on retrieved context and user query.\n",
    "    :param context: Retrieved documents.\n",
    "    :param question: User question.\n",
    "    :return: Generated response.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Generating response...\")\n",
    "    start_time = time.time()\n",
    "    docs = retriever.invoke(question)\n",
    "    formatted_context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Build prompt\n",
    "    full_prompt = prompt_template.format(question=question, context=formatted_context)\n",
    "\n",
    "    # Execution\n",
    "    response = chat(\n",
    "        model=\"mistral:7b\",\n",
    "        messages=[{'role': 'user', 'content': full_prompt}],\n",
    "        stream=False\n",
    "    )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[SUCCESS] Response generated. {elapsed_time:.2f}s\")\n",
    "    return StrOutputParser().parse(response[\"message\"][\"content\"])\n",
    "\n",
    "\n",
    "#### EXECUTION ####\n",
    "\n",
    "print(\"\\n#### STEP 1: Setup ####\")\n",
    "start_time = time.time()\n",
    "embedding_model = load_embedding_model()\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"[SUCCESS] {elapsed_time:.2f}s\")\n",
    "\n",
    "print(\"\\n#### STEP 2: Pre-Indexing ####\")\n",
    "start_time = time.time()\n",
    "doc_content = load_document('./assets/ressources/base.txt')\n",
    "chunks = chunk_text(doc_content)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"[SUCCESS] {elapsed_time:.2f}s\")\n",
    "\n",
    "print(\"\\n#### STEP 3: Indexing ####\")\n",
    "start_time = time.time()\n",
    "vectorstore = create_vectorstore_indexing_chunks(chunks, embedding_model)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"[SUCCESS] {elapsed_time:.2f}s\")\n",
    "\n",
    "print(\"\\n#### STEP 4: Retrieval & Generation ####\")\n",
    "start_time = time.time()\n",
    "question = \"Can you list the team members of M-Motors?\"\n",
    "retriever = create_retriever(vectorstore)\n",
    "prompt_template = define_prompt_template()\n",
    "response = generate_response(question, retriever, prompt_template)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"[SUCCESS] {elapsed_time:.2f}s\")\n",
    "\n",
    "print(\"\\n#### FINAL RESPONSE ####\")\n",
    "start_time = time.time()\n",
    "print(response)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"[SUCCESS] {elapsed_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb\n",
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### STEP 1: Setup ####\n",
      "[INFO] Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "[SUCCESS] Embedding model loaded. 8.03s\n",
      "[INFO] Creating ChromaDB database at ./chroma_db\n",
      "[SUCCESS] ChromaDB database initialized. 0.17s\n",
      "[SUCCESS] 8.20s\n",
      "\n",
      "#### STEP 2: Pre-Indexing ####\n",
      "[INFO] Loading document from ./assets/ressources/base.txt\n",
      "[SUCCESS] Document loaded. Length: 4388 characters. 0.00s\n",
      "[INFO] Chunking document...\n",
      "[SUCCESS] Document split into 6 chunks. 0.00s\n",
      "[SUCCESS] 0.00s\n",
      "\n",
      "#### STEP 3: Indexing ####\n",
      "[INFO] Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Add of existing embedding ID: 4\n",
      "Add of existing embedding ID: 5\n",
      "Insert of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 2\n",
      "Insert of existing embedding ID: 3\n",
      "Insert of existing embedding ID: 4\n",
      "Insert of existing embedding ID: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Generated 6 embeddings in 1.05s\n",
      "[INFO] Storing embeddings in ChromaDB...\n",
      "[SUCCESS] Documents stored in ChromaDB. 0.06s\n",
      "[SUCCESS] 1.10s\n",
      "\n",
      "#### STEP 4: Retrieval & Generation ####\n",
      "[INFO] Retrieving top-3 documents for query: Can you list the team members of M-Motors?\n",
      "[SUCCESS] Retrieved 3 documents. 0.04s\n",
      "[INFO] Generating response...\n",
      "[SUCCESS] Response generated. 199.26s\n",
      "[SUCCESS] 199.30s\n",
      "\n",
      "#### FINAL RESPONSE ####\n",
      " The team members of M-Motors are DIOUF Makhtar, Charlery Malcolm, RENÉ Marie, BENGUIGUI Avidan, RENEVIER Joachim, REKIK Kylian, and BERNARD Anne-Flore.\n",
      "[SUCCESS] 0.00s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import ollama\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#### SETUP ####\n",
    "\n",
    "def load_embedding_model(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Load the embedding model for text vectorization.\n",
    "    :param model_name: Name of the SentenceTransformer model to load.\n",
    "    :return: Loaded embedding model instance.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Loading embedding model: {model_name}\")\n",
    "    start_time = time.time()\n",
    "    model = SentenceTransformer(model_name)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[SUCCESS] Embedding model loaded. {elapsed_time:.2f}s\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_vector_database(persist_directory=\"./chroma_db\"):\n",
    "    \"\"\"\n",
    "    Initialize a persistent ChromaDB client and collection.\n",
    "    :param persist_directory: Path where the database is stored.\n",
    "    :return: ChromaDB collection instance.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Creating ChromaDB database at {persist_directory}\")\n",
    "    start_time = time.time()\n",
    "    client = chromadb.PersistentClient(path=persist_directory)\n",
    "    collection = client.get_or_create_collection(name=\"documents\")\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[SUCCESS] ChromaDB database initialized. {elapsed_time:.2f}s\")\n",
    "    return collection\n",
    "\n",
    "#### PRE-INDEXING ####\n",
    "\n",
    "def load_document(file_path):\n",
    "    \"\"\"\n",
    "    Load document content from a file.\n",
    "    :param file_path: Path to the document.\n",
    "    :return: String content of the file.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Loading document from {file_path}\")\n",
    "    start_time = time.time()\n",
    "    with open(file_path, 'r', encoding='utf-8') as fichier:\n",
    "        content = fichier.read()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[SUCCESS] Document loaded. Length: {len(content)} characters. {elapsed_time:.2f}s\")\n",
    "    return content\n",
    "\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    :param text: Input text.\n",
    "    :param chunk_size: Size of each chunk.\n",
    "    :param chunk_overlap: Overlap between consecutive chunks.\n",
    "    :return: List of text chunks.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Chunking document...\")\n",
    "    start_time = time.time()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - chunk_overlap \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[SUCCESS] Document split into {len(chunks)} chunks. {elapsed_time:.2f}s\")\n",
    "    return chunks\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "def create_embeddings(model, text_chunks):\n",
    "    \"\"\"\n",
    "    Generate embeddings for text chunks.\n",
    "    :param model: Embedding model.\n",
    "    :param text_chunks: List of text chunks.\n",
    "    :return: List of embeddings.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Generating embeddings...\")\n",
    "    start_time = time.time()\n",
    "    embeddings = model.encode(text_chunks).tolist()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[SUCCESS] Generated {len(embeddings)} embeddings in {elapsed_time:.2f}s\")\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def store_embeddings(collection, text_chunks, embeddings):\n",
    "    \"\"\"\n",
    "    Store embeddings and corresponding text chunks in ChromaDB.\n",
    "    :param collection: ChromaDB collection.\n",
    "    :param text_chunks: List of text chunks.\n",
    "    :param embeddings: List of embeddings.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Storing embeddings in ChromaDB...\")\n",
    "    start_time = time.time()\n",
    "    collection.add(\n",
    "        ids=[str(i) for i in range(len(text_chunks))],\n",
    "        documents=text_chunks,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[SUCCESS] Documents stored in ChromaDB. {elapsed_time:.2f}s\")\n",
    "\n",
    "#### RETRIEVAL ####\n",
    "\n",
    "def retrieve_documents(collection, model, query, top_k=3):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant documents for a given query.\n",
    "    :param collection: ChromaDB collection.\n",
    "    :param model: Embedding model.\n",
    "    :param query: Query string.\n",
    "    :param top_k: Number of documents to retrieve.\n",
    "    :return: List of retrieved documents.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Retrieving top-{top_k} documents for query: {query}\")\n",
    "    start_time = time.time()\n",
    "    query_embedding = model.encode([query]).tolist()[0]\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)\n",
    "    retrieved_docs = results[\"documents\"][0]\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[SUCCESS] Retrieved {len(retrieved_docs)} documents. {elapsed_time:.2f}s\")\n",
    "    return retrieved_docs\n",
    "\n",
    "#### GENERATION ####\n",
    "\n",
    "def generate_response(retrieved_docs, question):\n",
    "    \"\"\"\n",
    "    Generate a response based on retrieved context and user query.\n",
    "    :param context: Retrieved documents.\n",
    "    :param question: User question.\n",
    "    :return: Generated response.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Generating response...\")    \n",
    "    start_time = time.time()\n",
    "    formatted_context = \"\\n\\n\".join(retrieved_docs)\n",
    "    full_prompt = f\"Context:\\n{formatted_context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral:7b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "        stream=False\n",
    "    )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[SUCCESS] Response generated. {elapsed_time:.2f}s\")\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "#### EXECUTION ####\n",
    "\n",
    "print(\"\\n#### STEP 1: Setup ####\")\n",
    "start_time = time.time()\n",
    "embedding_model = load_embedding_model()\n",
    "collection = create_vector_database()\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"[SUCCESS] {elapsed_time:.2f}s\")\n",
    "\n",
    "print(\"\\n#### STEP 2: Pre-Indexing ####\")\n",
    "start_time = time.time()\n",
    "document = load_document(\"./assets/ressources/base.txt\")\n",
    "chunks = chunk_text(document)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"[SUCCESS] {elapsed_time:.2f}s\")\n",
    "\n",
    "print(\"\\n#### STEP 3: Indexing ####\")\n",
    "start_time = time.time()\n",
    "embeddings = create_embeddings(embedding_model, chunks)\n",
    "store_embeddings(collection, chunks, embeddings)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"[SUCCESS] {elapsed_time:.2f}s\")\n",
    "\n",
    "print(\"\\n#### STEP 4: Retrieval & Generation ####\")\n",
    "start_time = time.time()\n",
    "query = \"Can you list the team members of M-Motors?\"\n",
    "retrieved_docs = retrieve_documents(collection, embedding_model, query)\n",
    "response = generate_response(retrieved_docs, query)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"[SUCCESS] {elapsed_time:.2f}s\")\n",
    "\n",
    "print(\"\\n#### FINAL RESPONSE ####\")\n",
    "start_time = time.time()\n",
    "print(response)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"[SUCCESS] {elapsed_time:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
